{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embeddings of our dataset can be learned while training a neural network on the classification problem. Before it can be presented to the network, the text data is first encoded so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API provided with Keras. We add padding to make all the vectors of same length (max_length). Below code converts the text to integer indexes, now ready to be used in Keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noopa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\noopa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\noopa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\noopa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\noopa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\noopa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "total_reviews = X_train + X_test\n",
    "tokenizer_obj.fit_on_texts(total_reviews) \n",
    "\n",
    "# pad sequences\n",
    "max_length = 100 # try other options like mean\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "X_train_tokens =  tokenizer_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125602\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our neural network model. The model will use an Embedding layer as the first hidden layer. The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset during training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Summary of the built model...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          12560200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 12,577,257\n",
      "Trainable params: 12,577,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/25\n",
      " - 41s - loss: 0.5164 - accuracy: 0.7500 - val_loss: 0.4212 - val_accuracy: 0.8164\n",
      "Epoch 2/25\n",
      " - 41s - loss: 0.3038 - accuracy: 0.8820 - val_loss: 0.3953 - val_accuracy: 0.8331\n",
      "Epoch 3/25\n",
      " - 40s - loss: 0.2169 - accuracy: 0.9231 - val_loss: 0.4506 - val_accuracy: 0.8121\n",
      "Epoch 4/25\n",
      " - 40s - loss: 0.1664 - accuracy: 0.9421 - val_loss: 0.4813 - val_accuracy: 0.8200\n",
      "Epoch 5/25\n",
      " - 39s - loss: 0.1515 - accuracy: 0.9492 - val_loss: 0.5177 - val_accuracy: 0.8125\n",
      "Epoch 6/25\n",
      " - 40s - loss: 0.1110 - accuracy: 0.9646 - val_loss: 0.5468 - val_accuracy: 0.8128\n",
      "Epoch 7/25\n",
      " - 40s - loss: 0.0817 - accuracy: 0.9748 - val_loss: 0.6387 - val_accuracy: 0.8098\n",
      "Epoch 8/25\n",
      " - 40s - loss: 0.0725 - accuracy: 0.9773 - val_loss: 0.6860 - val_accuracy: 0.7903\n",
      "Epoch 9/25\n",
      " - 39s - loss: 0.0788 - accuracy: 0.9750 - val_loss: 0.6092 - val_accuracy: 0.8180\n",
      "Epoch 10/25\n",
      " - 40s - loss: 0.0538 - accuracy: 0.9836 - val_loss: 0.6740 - val_accuracy: 0.8138\n",
      "Epoch 11/25\n",
      " - 39s - loss: 0.0444 - accuracy: 0.9874 - val_loss: 0.7024 - val_accuracy: 0.8101\n",
      "Epoch 12/25\n",
      " - 39s - loss: 0.0326 - accuracy: 0.9913 - val_loss: 0.7460 - val_accuracy: 0.8142\n",
      "Epoch 13/25\n",
      " - 40s - loss: 0.0380 - accuracy: 0.9881 - val_loss: 0.7572 - val_accuracy: 0.8043\n",
      "Epoch 14/25\n",
      " - 41s - loss: 0.0369 - accuracy: 0.9886 - val_loss: 0.7713 - val_accuracy: 0.8085\n",
      "Epoch 15/25\n",
      " - 40s - loss: 0.0312 - accuracy: 0.9903 - val_loss: 0.8051 - val_accuracy: 0.7989\n",
      "Epoch 16/25\n",
      " - 40s - loss: 0.0251 - accuracy: 0.9928 - val_loss: 0.8461 - val_accuracy: 0.8021\n",
      "Epoch 17/25\n",
      " - 40s - loss: 0.0447 - accuracy: 0.9878 - val_loss: 0.6604 - val_accuracy: 0.7983\n",
      "Epoch 18/25\n",
      " - 40s - loss: 0.0499 - accuracy: 0.9842 - val_loss: 0.7827 - val_accuracy: 0.8095\n",
      "Epoch 19/25\n",
      " - 40s - loss: 0.0314 - accuracy: 0.9907 - val_loss: 0.8616 - val_accuracy: 0.8077\n",
      "Epoch 20/25\n",
      " - 41s - loss: 0.0191 - accuracy: 0.9950 - val_loss: 0.8999 - val_accuracy: 0.8045\n",
      "Epoch 21/25\n",
      " - 41s - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.9041 - val_accuracy: 0.8073\n",
      "Epoch 22/25\n",
      " - 42s - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.9369 - val_accuracy: 0.8024\n",
      "Epoch 23/25\n",
      " - 41s - loss: 0.0152 - accuracy: 0.9962 - val_loss: 0.9703 - val_accuracy: 0.7990\n",
      "Epoch 24/25\n",
      " - 41s - loss: 0.0140 - accuracy: 0.9960 - val_loss: 1.0121 - val_accuracy: 0.8024\n",
      "Epoch 25/25\n",
      " - 40s - loss: 0.0341 - accuracy: 0.9902 - val_loss: 0.9582 - val_accuracy: 0.7932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x138801c1808>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "25000/25000 [==============================] - 3s 116us/step\n",
      "Test score: 0.958162715177536\n",
      "Test accuracy: 0.7931600213050842\n",
      "Accuracy: 79.32%\n"
     ]
    }
   ],
   "source": [
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
